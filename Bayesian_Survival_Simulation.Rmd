---
title: "Bayesian Simulation"
author: "Yufang Wang"
date: "2025-03-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=TRUE}
rm(list = ls())
setwd("G:/MasterThesis/Code")#G:\MasterThesis\Code

# Function to check and install packages
install_and_load <- function(packages) {
  for (pkg in packages) {
    if (!require(pkg, character.only = TRUE)) {
      install.packages(pkg, dependencies = TRUE)
    }

    library(pkg, character.only = TRUE) }
  
  
}

# List of required packages
required_packages <- c("rstan", "readr", "survival", "splines2", "dplyr", "simsurv", "SurvMetrics")

# Install and/or load packages
install_and_load(required_packages)

# Set the seed for reproducable research
set.seed(42)
```

# Method 1: Semiparametric Bayesian survival model (Partical likelihood, in the case of being only interested in the p values of X)
```{r, echo=TRUE}
Bayesian_Survival_PH <- function(stan_data,
                              niter = 10000,
                              nwarmup = 1000,
                              thin = 10,
                              chains = 1) {
  # Proportional Hazard (PH) Model with covariates via partial likelihood function
  # compile the model
  bayesian_model <- stan_model("PH.stan")
  
  # Model fitting and summary
  bayesian_model_fit <- suppressWarnings(
    sampling(
      bayesian_model,
      data = stan_data,
      iter = niter,
      warmup = nwarmup,
      thin = 10,
      chain = 1
    )
  )
  
  # Summary of the fit
  output <- summary(bayesian_model_fit)$summary
  return(output)
}

```

# Method 2: Paramertic (Full liklihood) Bayesian Survival Model (in the case of being interested in prediction, inference & variable selection)

## Methodological techniques
- The **Baseline hazard function** was evaluated via three following approaches: a. under the assumption of exponential distribution; b. under the assumption of weibull distribution; c. assumption-free, i.e., using the *B-splines* method to estimate the baseline function. In this case, the **Cumulative hazard function** was estimated by the gauss_kronrod quadradtic intergal. From the time points that does not exist, linear interpolation was also used. 

- LogLiklihood function =  sum(-H_i + logh_i) for uncensored data + sum(-H_i) for censored data.

## Evaluation techniques
- The **prediction** is estimated at the level of Intergral brier score. In the case of testing dataset and training dataset were observed at different time points, we used the linear interpolation to get the estimated survival probability at the time points that only occurs in the testing dataset.

```{r, echo=TRUE}
# Exponential baseline distribution from a bayesian perspective
Bayesian_Survival_includingbaseline <- function(stan_data, baseline_assumption = "exponential", school = "Bayesian", 
                              niter = 10000, 
                              nwarmup = 1000,
                              thin = 10,
                              chains = 1) {
  if (school == "Bayesian") {
    if (baseline_assumption == "exponential") {
      # compile the model
      bayesian_model <- stan_model("./exponential.stan")
    }
    
    else if (baseline_assumption == "weibull") {
      # compile the model
      bayesian_model <- stan_model("./weibull.stan")
    }
    
    else if (baseline_assumption == "bSplines") {
      message("We utilized B-splines to estimate the baseline cumulative hazard function.")
      time_combined <- sort(unique(c(stan_data$t, stan_data$t_cens)))
      notes = sort(runif(5, min(time_combined), max(time_combined)))
      bSpline_basis <- bSpline(time_combined, knots = notes, degree = 1, intercept = FALSE) # The B-spline basis is calculated using the method implemented in the splines2 package
      
      
      # Out the corresponding information in stan data
      stan_data$bSpline_basis <- bSpline_basis
      stan_data$M = length(time_combined)
      stan_data$uniqueT = time_combined
      
      library(mvQuad)
      
      # Create a Gauss-Kronrod grid
      grid <- createNIGrid(dim = 1, type = "GHe", level = 15)
      
      # Get nodes and weights
      locates <- getNodes(grid)
      weights <- getWeights(grid)
      stan_data$locates <-locates
      stan_data$weights <- weights
      
      # compile the model
      bayesian_model <- stan_model("./bSpline_model.stan")
    }
    
    # Model fitting and summary
    bayesian_model_fit <- suppressWarnings(
      sampling(
        bayesian_model,
        data = stan_data,
        iter = niter,
        warmup = nwarmup,
        thin = 10,
        chain = 1
      )
    )
    
    return(bayesian_model_fit)
  }
  }
```

# train-test dataset splitting
Such splitting does not account for the case of non-overlapped time-points because of the proportional hazard assumption
```{r, echo=TRUE}
n_samples = 1000
covariates <- data.frame(
  id = 1:n_samples,  # 100 individuals
  cov1 = rbinom(n_samples, 1, 0.5),  # First covariate
  cov2 = rnorm(n_samples, mean = 0, sd = 1)   # Second covariate
)

# Define the effects of the covariates
beta <- c(cov1 = -0.5, cov2 = 0.3)  # Coefficients for cov1 and cov2


# Simulate the survival data
sim_data <- simsurv(
  dist = "weibull",
  lambdas = 2, # scale
  gammas = 5, # shape for weibull
  betas = beta,
  x = covariates,
  mixture = FALSE,
  maxt = 2  # Maximum follow-up time
)

# Print simulated data
head(sim_data)
library(dplyr)
sim_data <- cbind(sim_data, covariates) |> select("id", "eventtime", "status", "cov1", "cov2")
censtime <- runif(n_samples, 3, 10)
status <- as.numeric(sim_data$eventtime <= censtime) # the sum of status is the number of event
sim_data$obstime = sim_data$eventtime * status + censtime*(1-status)
sim_data$status1 = status
head(sim_data)

# Define the cross-validation
CrossValidation <- function(dataset = sim_data, KFold = 5, FDR = 0.1, Beta_real = beta, simulated = TRUE){
  #pre-allocate the variables 
  rMSE = matrix(NA, nrow = KFold, ncol = sum(!(names(dataset) %in% c("id", "eventtime", "status", "obstime", "status1"))) )
  IBrier_scores = numeric(KFold)
  
  # shuffled_indices <- sample(seq_len(nrow(dataset)))
  # folds <- cut(seq_along(shuffled_indices), breaks = KFold, labels = FALSE)
  # folds <- cut(seq(1:nrow(dataset)), breaks = KFold, labels = FALSE)
  
  for (i in 1:KFold){
    train_id = sample(1:nrow(dataset), size = 0.8*nrow(dataset))
    train_dataset = dataset[train_id,]
    test_dataset = dataset[-train_id,]
    
    #build up the list data to fit the bayesian stan model
    stan_data <- list(
      # Data for Bayesian model estimation
      K =  sum(!(names(dataset) %in% c("id", "eventtime", "status", "obstime", "status1"))),
      # 2 represents the columns of t and status
      N = nrow(train_dataset[train_dataset$status == 1,]),
      # Number of non-censored data
      t = train_dataset[train_dataset$status == 1, "obstime"],
      x = train_dataset[train_dataset$status == 1, !(names(train_dataset) %in% c("id", "eventtime", "status", "obstime", "status1"))],
      
      N_cens = nrow(train_dataset[train_dataset$status == 0,]),
      # Number of censored data
      t_cens = train_dataset[train_dataset$status == 0, "obstime"],
      x_cens = train_dataset[train_dataset$status == 0,!(names(train_dataset) %in% c("id", "eventtime", "status",  "obstime", "status1"))],
      
      # Data for Bayesian model prediction, issues might pop up during brier score calculation if the status for t' (test dataset) does not exist in t (train dataset)
      
      N_new = nrow(test_dataset),
      x_new = test_dataset[,!(names(test_dataset) %in% c("id", "eventtime", "status",  "obstime", "status1"))],
      t_new = train_dataset[, "obstime"],
      status_new = train_dataset[, "status1"]
    )
    
    # fit the model
    model <- Bayesian_Survival_includingbaseline(stan_data = stan_data, baseline_assumption = "bSplines") # #exponential, weibull,  bSplines
    
    if (simulated == TRUE){
      # EVAL 1: evaluating the model performance at the level at the level of Beta's.
      Beta_bayesian_est = Output[grep("^Beta", rownames(Output)), "mean", drop = FALSE] ## Extract the Betas
      MSE_est = (Beta_bayesian_est - Beta_real) ** 2 # MSE_est is a vector with the length being number of Betas
      rMSE[i,] = MSE_est
      
      # EVAL 2: evaluating the model performance at the level of prediction
      sp = Output[grep("^survival_prob", rownames(Output)), "mean", drop = FALSE]
      sp_matrix = matrix(sp, ncol = nrow(test_dataset)) #time points * Nobs_testing
      sp_matrix = t(sp_matrix) # after tranpose, sp_matrix will be Nobs_testing * time points
      training_tpoints <- sort(unique(train_dataset$obstime))
      
      testing_tpoints <- sort(unique(test_dataset$obstime))
      sp_matrix_realT = matrix(NA,
                               nrow = nrow(test_dataset),
                               ncol = length(testing_tpoints))
      
      # use the approxfun() function in R to interpolate the time points that actually observed for the test dataset
      for (obsID in 1:nrow(test_dataset)) {
        survival_fun = approxfun(training_tpoints, sp_matrix[obsID, ], method = "linear")
        sp_matrix_realT[obsID, ] = survival_fun(testing_tpoints)
      }
      SurvObj = Surv(test_dataset$eventtime, test_dataset$status)
      
      # interpolation for the time points only occurs in the test dataset
      IBrier_score = IBS(SurvObj, sp_matrix_realT, test_dataset$eventtime) #IBrier-score is a scaler
      IBrier_scores[i] = IBrier_score
      
      # EVAL 3: evaluating the model variable selection
      #Beta_bayesian_interval = Output[grep("^Beta", rownames(Output)), c("mean", ""), drop = FALSE]
      
    } 
    else {
      # EVAL 1: evaluating the model performance at the level at the level of Beta's.
      Beta_bayesian_est = Output[grep("^Beta", rownames(Output)), "mean", drop = FALSE] ## Extract the Betas
      
      
      message("In the case of real-world data, we used the Cox regression estimator as real Beta")
      cox_model = coxph(Surv(eventtime, status) ~ cov1 + cov2, data = train_dataset)
      Beta_cox_est = coef(cox_model)
      MSE_est = (Beta_bayesian_est - Beta_cox_est) ** 2 # MSE_est is a vector with the length being number of Betas
      rMSE[i,] = MSE_est
      
      # EVAL 2: evaluating the model performance at the level of prediction
      sp = Output[grep("^survival_prob", rownames(Output)), "mean", drop = FALSE]
      sp_matrix = matrix(sp, nrow = nrow(test_dataset))#time points * Nobs_testing
      sp_matrix = t(sp_matrix) # after tranpose, sp_matrix will be Nobs_testing * time points
      training_tpoints <- sort(unique(train_dataset$obstime))
      
      testing_tpoints <- sort(unique(test_dataset$obstime))
      sp_matrix_realT = matrix(NA,
                               nrow = nrow(test_dataset),
                               ncol = length(testing_tpoints))
      
      # use the approxfun() function in R to interpolate the time points that actually observed for the test dataset
      for (obsID in 1:nrow(test_dataset)) {
        survival_fun = approxfun(training_tpoints, sp_matrix[obsID, ], method = "linear")
        sp_matrix_realT[obsID, ] = survival_fun(testing_tpoints)
      }
      SurvObj = Surv(test_dataset$eventtime, test_dataset$status)
      
      # interpolation for the time points only occurs in the test dataset
      IBrier_score = IBS(SurvObj, sp_matrix_realT, test_dataset$eventtime) #IBrier-score is a scaler
      IBrier_scores[i] = IBrier_score
      
      # EVAL 3: evaluating the model variable selection
      message("we cannot evaluate the model regarding variable selection")
    }
  }
}

CrossValidation()
```
