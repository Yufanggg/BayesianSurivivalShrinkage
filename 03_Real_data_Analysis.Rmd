---
title: "01_Sim_data_Analysisbaseline"
author: "Yufang Wang"
date: "2025-03-18"
output: pdf_document
---

# Step 1: Set up the environment
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
setwd("C:/Users/Alisa_Wang/Desktop/MasterThesis/Code/")#C:/Users/Alisa_Wang/Desktop/MasterThesis/Code/")#G:/MasterThesis/Code/"
source("./function_file/Functions.R")

install_and_load <- function(packages) {
  for (pkg in packages) {
    if (!require(pkg, character.only = TRUE)) {
      install.packages(pkg, dependencies = TRUE)
    }
    library(pkg, character.only = TRUE) }
  
}

# List of required packages
required_packages <- c("rstan", "MASS", "rstantools", "coda", "readr", "survival", "splines2", "dplyr", "simsurv", "Hmisc", "caret")
# Install and/or load packages
install_and_load(required_packages)
```


# Step 2: Organize the real-world data
```{r, echo=TRUE}
real_df <- readRDS("./Data/preprocessed_NOTR_DGF.rds")
sum(complete.cases(real_df))
str(real_df)
real_df <- subset(real_df, select = -c(Typeofdonor, InitialPrimaryDiseaseET, RecipientfullmatchHLAstring))

#sum-coded all factor variables
factor_var <- sapply(real_df, is.factor)
for (var in names(real_df)[factor_var]){
  if (var != "status"){
    contrasts(real_df[[var]]) <- contr.sum(levels(real_df[[var]]))
  }
}
# lapply(real_df[, factor_var], contrasts)


predictors <- subset(real_df, select = -c(status, time))

interaction_formula <- as.formula(paste("~ (", paste(colnames(predictors), collapse = " + "), ")^2"))
interaction_matrix <- model.matrix(interaction_formula, data = real_df)

interaction_df <- as.data.frame(interaction_matrix)
interaction_df$id = 1:nrow(interaction_df) #real_df$ID
interaction_df$status = ifelse(real_df$status== "graftloss", 1, 0) # death  censored graftloss
interaction_df$obstime =  real_df$time
obs_window = 3415 #max(df_using$time_tx_date_max2)
interaction_df = interaction_df[-1]
```

# estimate the Betas
```{r, echo=TRUE}
# only for estimating the Betas
stan_data_bSplines <- stan_data_Constructer(
      training_dataset = interaction_df,
      baseline_modelling = "bSplines",
      obs_window = obs_window
    )

model_fit_bSplines <- Bayesian_Survival_model(stan_data = stan_data_bSplines, withPrediction = FALSE, baseline_assumption = "bSplines")

#model diagnotics
diagnostics_bSplines <- summary(model_fit_bSplines)$summary[, c("Rhat", "n_eff")]

#extract the info from the bayesian model fit
model_result_bSplines <- Bayesian_Survival_result_Extract(bayesian_model_fit = model_fit_bSplines, model_type = "bSplines", criteria = c("DesignCoefficients", "baseline", "variableSelection"))

fit = model_result_bSplines$
mean_beta = model_result_bSplines$Beta_bayesian_est[, "mean"]
FDR_beta = model_result_bSplines$variableSelection
```

# Cross-validation for Predictions
```{r, echo=TRUE}
cross_validation <- function(whole_dataset, baseline_modelling = "weibull", num_folds = 5){
  # create the cross-validation folds
  folds <- createFolds(whole_dataset$status, k = num_folds, list = TRUE, returnTrain = FALSE)
  cross_val_metric <- list()
  rMSE_s = matrix(nrow = 5, ncol = 55); Brier_scores =  rep(NA, 5); C_indices =  rep(NA, 5); FDR_vals =  rep(NA, 5)
  for (i in 1:num_folds){
    fold_indices <- folds[[i]]
    training_data = whole_dataset[-fold_indices, ]
    testing_data = whole_dataset[fold_indices, ]
    stan_data <- stan_data_Constructer(training_dataset = training_data, testing_dataset = testing_data, baseline_modelling = baseline_modelling, obs_window = 5)
    model_fit <- Bayesian_Survival_model(stan_data = stan_data, baseline_assumption = baseline_modelling)
    
    #extract the info from the bayesian model fit
    model_result <- Bayesian_Survival_result_Extract(bayesian_model_fit = model_fit)
    
    ComparsionValues <- list(
      Beta_reference = c(beta, rep(0, 45)),
      test_t = testing_data$obstime,
      test_status = testing_data$status,
      Selection_reference = c(rep(TRUE, 10), rep(FALSE, 45))
      )
    model_metric <- Model_performance_eval(model_result = model_result, ComparsionValues)
    rMSE_s[i, ] <- model_metric$rMSE
    Brier_scores[i] <- model_metric$Brier_score
    C_indices[i] <- model_metric$C_index
    FDR_vals[i] <- model_metric$FDR
  }
  cross_val_metric$rMSE_s <- rMSE_s
  cross_val_metric$Brier_scores <- Brier_scores
  cross_val_metric$C_indices <- C_indices
  cross_val_metric$FDR_vals <- FDR_vals
  
  return(cross_val_metric)
}

cross_val_metric <- cross_validation(whole_dataset = sim_data, baseline_modelling = "bSplines") # exponential, weibull, bSplines

# Step 3: visualize the model metric results
metric_visualization(cross_val_metric)






  if ("Prediction_SurvivalProb" %in% criteria) {
    
    output$time = testing_dataset$obstime
    output$true_status = testing_dataset$status
    output$sp_exponential = model_result_exponential$sp
    output$sp_weibull = model_result_weibull$sp
    output$sp_bSplines = model_result_bSplines$sp
  }

}

```


# Step 4: use cross validation for prediction.
```{r, echo=TRUE}
cross_validation <- function(whole_dataset, baseline_modelling = "weibull", num_folds = 5){
  # create the cross-validation folds
  folds <- createFolds(whole_dataset$status, k = num_folds, list = TRUE, returnTrain = FALSE)
  cross_val_metric <- list()
  rMSE_s = matrix(nrow = 5, ncol = 55); Brier_scores =  rep(NA, 5); C_indices =  rep(NA, 5); FDR_vals =  rep(NA, 5)
  for (i in 1:num_folds){
    fold_indices <- folds[[i]]
    training_data = whole_dataset[-fold_indices, ]
    testing_data = whole_dataset[fold_indices, ]
    stan_data <- stan_data_Constructer(training_dataset = training_data, testing_dataset = testing_data, baseline_modelling = baseline_modelling, obs_window = 5)
    model_fit <- Bayesian_Survival_model(stan_data = stan_data, baseline_assumption = baseline_modelling)
    
    #extract the info from the bayesian model fit
    model_result <- Bayesian_Survival_result_Extract(bayesian_model_fit = model_fit)
    
    ComparsionValues <- list(
      Beta_reference = c(beta, rep(0, 45)),
      test_t = testing_data$obstime,
      test_status = testing_data$status,
      Selection_reference = c(rep(TRUE, 10), rep(FALSE, 45))
      )
    model_metric <- Model_performance_eval(model_result = model_result, ComparsionValues)
    rMSE_s[i, ] <- model_metric$rMSE
    Brier_scores[i] <- model_metric$Brier_score
    C_indices[i] <- model_metric$C_index
    FDR_vals[i] <- model_metric$FDR
  }
  cross_val_metric$rMSE_s <- rMSE_s
  cross_val_metric$Brier_scores <- Brier_scores
  cross_val_metric$C_indices <- C_indices
  cross_val_metric$FDR_vals <- FDR_vals
  
  return(cross_val_metric)
}

cross_val_metric <- cross_validation(whole_dataset = sim_data, baseline_modelling = "bSplines") # exponential, weibull, bSplines

# Step 3: visualize the model metric results
metric_visualization(cross_val_metric)
```

